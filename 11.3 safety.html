<!doctype html>
<html>
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
 <link href="/src/output.css" rel="stylesheet">


</head>
  <body class = "bg-[#d4f0ff]"> 
    <article class="marosemain">
       <h1><strong>11.3 SAFETY</strong></h1>
    </article>

    <div class="navbar">
        <a class="navbar-text" href="Index.html">Home</a>
        <a class="navbar-text" href="Introduction.html">Introduction</a>
        <a class="navbar-text" href="chapter 11.1.html">Dependability properties</a>
        <a class="navbar-text" href="11.2.html">Cost/dependability curve</a>
        <a class=" active navbar-text" href="11.3 safety.html">Safety</a>
        <a class="navbar-text" href="11.4 - Security.html">Security</a> 
    </div>

  <div class="marosetitle">
    

    <p>Safety-critical systems are systems where it is essential that system operation is
always safe; that is, the system should never damage people or the system’s environment even if the system fails. Examples of safety-critical systems include control and monitoring systems in aircraft, process control systems in chemical and
pharmaceutical plants, and automobile control systems.
Hardware control of safety-critical systems is simpler to implement and analyze
than software control. However, we now build systems of such complexity that they
cannot be controlled by hardware alone. Software control is essential because of the
need to manage large numbers of sensors and actuators with complex control laws. For
example, advanced, aerodynamically unstable, military aircraft require continual
software-controlled adjustment of their flight surfaces to ensure that they do not crash.</p>
 <article>

  <b><h2>Safety-critical software falls into two classes:</h2></b>


  <p><ol>1.Primary safety-critical software This is software that is embedded as a controller in a system. Malfunctioning of such software can cause a hardware
malfunction, which results in human injury or environmental damage. The
insulin pump software, introduced in Chapter 1, is an example of a primary
safety-critical system. System failure may lead to user injury.</ol></p>
  <p><ol>2. Secondary safety-critical software This is software that can indirectly result in
an injury. An example of such software is a computer-aided engineering design
system whose malfunctioning might result in a design fault in the object being
designed. This fault may cause injury to people if the designed system malfunctions. Another example of a secondary safety-critical system is the mental
health care management system, MHC-PMS. Failure of this system, whereby an
unstable patient may not be treated properly, could lead to that patient injuring
themselves or others.
</ol></p>
 
  <p>System reliability and system safety are related but a reliable system can be
unsafe and vice versa. The software may still behave in such a way that the resultant
system behavior leads to an accident. There are four reasons why software systems
that are reliable are not necessarily safe:</p>
  <p><ol>1. We can never be 100% certain that a software system is fault-free and faulttolerant. Undetected faults can be dormant for a long time and software
failures can occur after many years of reliable operation.</ol></p>
  <p><ol>2. The specification may be incomplete in that it does not describe the required
behavior of the system in some critical situations. A high percentage of system
malfunctions (Boehm et al., 1975; Endres, 1975; Lutz, 1993; Nakajo and Kume,
1991) are the result of specification rather than design errors. In a study of errors
in embedded systems, Lutz concludes:
</ol></p>
  <p>. . . difficulties with requirements are the key root cause of the safetyrelated software errors, which have persisted until integration and system
testing.
</p>
  <p><ol>3. Hardware malfunctions may cause the system to behave in an unpredictable
way, and present the software with an unanticipated environment. When components are close to physical failure, they may behave erratically and generate
signals that are outside the ranges that can be handled by the software.</ol></p>
</main>
</div>
   </main>

       <h2><section><strong class="marosebody">Figure 11.6</strong></section></h2>   
      <h5><section><b>Safety terminology</b></section></h5>
       <div>
  

  <div class="marosebody2">
    
    <p><ol>4.The system operators may generate inputs that are not individually incorrect but
which, in some situations, can lead to a system malfunction. An anecdotal
example of this occurred when an aircraft undercarriage collapsed whilst the
aircraft was on the ground. Apparently, a technician pressed a button that
instructed the utility management software to raise the undercarriage. The software carried out the mechanic’s instruction perfectly. However, the system
should have disallowed the command unless the plane was in the air.</ol></p>
    <p><article>A specialized vocabulary has evolved to discuss safety-critical systems and it is
important to understand the specific terms used. Figure 11.6 summarizes some definitions of important terms, with examples taken from the insulin pump system.
The key to assuring safety is to ensure either that accidents do not occur or that
the consequences of an accident are minimal. This can be achieved in three complementary ways:
</article></p>
    <p><article>The key to assuring safety is to ensure either that accidents do not occur or that
the consequences of an accident are minimal. This can be achieved in three complementary ways:</article></p>
    <p><ol>1. Hazard avoidance The system is designed so that hazards are avoided. For
example, a cutting system that requires an operator to use two hands to press separate buttons simultaneously avoids the hazard of the operator’s hands being
in the blade pathway.</ol></p>
    <p><ol>2. Hazard detection and removal The system is designed so that hazards are
detected and removed before they result in an accident. For example, a chemical
plant system may detect excessive pressure and open a relief valve to reduce
these pressures before an explosion occurs</ol></p>
    <p><ol>3.Damage limitation The system may include protection features that minimize
the damage that may result from an accident. For example, an aircraft engine
normally includes automatic fire extinguishers. If a fire occurs, it can often be
controlled before it poses a threat to the aircraft.</ol></p>


  <section>
    <article>Accidents most often occur when several things go wrong at the same time. Ananalysis of serious accidents (Perrow, 1984) suggests that they were almost all due toa combination of failures in different parts of a system. Unanticipated combinationsof subsystem failures led to interactions that resulted in overall system failure. Forexample, failure of an air-conditioning system could lead to overheating, which thencauses the system hardware to generate incorrect signals. Perrow also suggests that itis impossible to anticipate all possible combinations of failures. Accidents are therefore an inevitable part of using complex systems.</article>
    <article>Some people have used this as an argument against software control. Because ofthe complexity of software, there are more interactions between the different parts ofa system. This means that there will probably be more combinations of faults thatcould lead to system failure</article>
    <article>However, software-controlled systems can monitor a wider range of conditionsthan electro-mechanical systems. They can be adapted relatively easily. They usecomputer hardware, which has very high inherent reliability and which is physicallysmall and lightweight. Software-controlled systems can provide sophisticated safetyinterlocks. They can support control strategies that reduce the amount of time peopleneed to spend in hazardous environments. Although software control may introducemore ways in which a system can go wrong, it also allows better monitoring and protection and hence can contribute to improvements in system safety.</article>
    <article>In all cases, it is important to maintain a sense of proportion about system safety.It is impossible to make a system 100% safe and society has to decide whether or notthe consequences of an occasional accident are worth the benefits that come from theuse of advanced technologies. It is also a social and political decision about how todeploy limited national resources to reduce risk to the population as a whole.</article>
  </section>

</body>
</html>